{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "from utils.misc_utils import *\n",
    "from utils.load_and_preprocessing_utils import load_img_pil\n",
    "import PIL\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"Device\",device)\n",
    "\n",
    "all_models = [\"stabilityai/stable-diffusion-2-inpainting\",\n",
    "\"diffusionbee/fooocus_inpainting\",\n",
    "\"Vijish/fooocus_inpainting\",\n",
    "\"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
    "'/home/waleed/my_stuff/codes/my_testing/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors',\n",
    "\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='pretrained_models/blaze_face_short_range.tflite')\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_alignment\n",
    "FA_model = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, flip_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"Device\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SD Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc_utils import clear_memory\n",
    "from utils.sd_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = [\"stabilityai/stable-diffusion-2-inpainting\",\n",
    "\"diffusionbee/fooocus_inpainting\",\n",
    "\"Vijish/fooocus_inpainting\",\n",
    "\"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
    "'../../pretrained_models/diffusion/civitai/juggernautXL_versionXInpaint.safetensors',\n",
    "\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clear_memory()\n",
    "\n",
    "# all_model_types = [\"standard\",\"inpainting\",\"xl\",\"xl-inpainting\"]\n",
    "# model_path = all_models[-1]\n",
    "# model_type = all_model_types[-2]\n",
    "\n",
    "# print('Model:',model_path)\n",
    "# print('Type:',model_type)\n",
    "# pipe = load_diffusion_model(model_path, model_type=model_type)\n",
    "# clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.enable_sequential_cpu_offload();\n",
    "# pipe = pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of LoRAs\n",
    "# lora_sources = [\n",
    "#     {   \n",
    "#         \"name\":\"photo_lora\",\n",
    "#         \"path\": \"../my_testing/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors\",\n",
    "#         \"weight\": 0.5\n",
    "#     },\n",
    "#     {   \n",
    "#         \"name\":\"toy_face\",\n",
    "#         \"path\": \"CiroN2022/toy-face\",\n",
    "#         \"weight_name\": \"toy_face_sdxl.safetensors\",\n",
    "#         \"weight\": 0.5\n",
    "#     },\n",
    "#     {   \n",
    "#         \"name\":\"pixel_art\",\n",
    "#         \"path\": \"nerijs/pixel-art-xl\",\n",
    "#         \"weight_name\": \"pixel-art-xl.safetensors\",\n",
    "#         \"weight\": 0.5\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# pipe = load_loras_into_pipeline(pipe, lora_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sd_utils import perform_inpainting, perform_diffusion\n",
    "from utils.misc_utils import make_folders_multi,clear_memory\n",
    "import torch\n",
    "from utils.mp_landmark_ids import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# path_dataset = '../../datasets/test_images_side/'\n",
    "path_dataset = '../../datasets/testCase_images/before/'\n",
    "all_paths = glob(os.path.join(path_dataset,'*'))\n",
    "all_paths.sort()\n",
    "# all_paths = ['3-1_FL5_intensityLevel3_LateralRight.png']\n",
    "\n",
    "print('Total Images:',len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "# img_path = all_paths[-7]\n",
    "path_results = 'inpaint_results/'\n",
    "\n",
    "landmark_nose = [32,30,29,28,27,21]\n",
    "masked_parts_list = [mp_nose]\n",
    "# masked_parts_list = [landmark_nose]#[mp_nose,mp_cheeks_L,mp_cheeks_R,mp_chin]\n",
    "\n",
    "dilate_list = [10]\n",
    "img_shape=(1024,1024)\n",
    "save_quality = 95\n",
    "face_front = True    \n",
    "\n",
    "## SD Settings\n",
    "# seed = torch.seed()\n",
    "seed = 0\n",
    "generator = torch.manual_seed(seed)  # Create a generator with a fixed seed\n",
    "print(seed)\n",
    "\n",
    "num_steps = 30\n",
    "num_imgs = 1\n",
    "guidance_scale = 7.5\n",
    "strength = 1.0\n",
    "lora_scale = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"close-up portrait of a woman with a prominent hawk nose, strong roman nose, curved nose bridge, pronounced nasal ridge, realistic skin texture, visible pores, natural skin, photorealistic, cinematic lighting, ultra-detailed skin, side light, 8k\"\n",
    "# prompt = \"close-up portrait of a woman with a prominent hawk nose, big nose hump, realistic skin texture, visible pores, natural skin, photorealistic, cinematic lighting, ultra-detailed skin, side light, 8k\"\n",
    "# prompt = \"side profile portrait of a person with a small, upturned button nose, sharp nose tip, smooth nasal bridge, photorealistic, realistic skin texture, visible pores, ultra-detailed skin, soft cinematic lighting, 8k\"\n",
    "\n",
    "# prompt = \"side profile portrait of a person with button nose, upturned nasal tip, upturned button nose\"\n",
    "# negative_prompt = \"blurry, soft skin, smooth, airbrushed, waxy, cartoon, deformed, unnatural, filter, makeup, flawless skin, deformed, disfigured\"\n",
    "\n",
    "# before = \"side profile of a person, hyperrealistic side profile portrait, photorealistic skin texture, visible pores, ultra detailed, dramatic lighting, sharp focus, 8k, shallow depth of field\"\n",
    "# trigger = \"film photography style light grain medium grain heavy grain\"\n",
    "# prompt = f\"{before}, {prompt}, {trigger}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # before = \"hyperrealistic portrait, photorealistic skin texture, visible pores, ultra detailed, dramatic lighting, sharp focus, 8k, shallow depth of field\"\n",
    "# # nose_type = \"portrait of a person with a sharply curved hawk nose, prominent convex bridge, pointed nasal tip\"\n",
    "# # prompt = f\"{before}, {nose_type}\"\n",
    "\n",
    "# # negative_prompt = \"blurry, soft skin, smooth, airbrushed, waxy, cartoon, deformed, unnatural, filter, makeup, flawless skin\"\n",
    "\n",
    "# prompt = \"hacker with a hoodie, film photography style , toy_face , pixel art\"\n",
    "# negative_prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.set_adapters([\"photo_lora\",\"toy\",\"pixel\"], adapter_weights=[0,0,1.0])\n",
    "# pipe.set_adapters([\"pixel\"], adapter_weights=[1])\n",
    "\n",
    "# # # pipe.set_adapters(\"photo_lora\")\n",
    "# pipe.disable_lora()\n",
    "# pipe.enable_lora()\n",
    "# # pipe.delete_adapters(\"toy\")\n",
    "# # pipe.delete_adapters(\"photo_lora\")\n",
    "# # pipe.delete_adapters(\"pixel\")\n",
    "# # pipe.disable_lora()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\", adapter_name=\"pixel\")\n",
    "# pipe.set_adapters([\"pixel\"], adapter_weights=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.disable_lora()\n",
    "# pipe.enable_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.get_active_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = '/home/waleed/Downloads/man_test.png'\n",
    "# all_paths = [img_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'A small, delicate nose with a slightly upturned tip, feminine features on a male face, pixel art'\n",
    "# negative_prompt = 'Large pores, masculine nose shape, rugged texture, disproportionate size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_scale = 1.0\n",
    "# generated_img = perform_inpainting(pipe,FA_model,detector,all_paths,path_results,masked_parts_list,dilate_list,seed,prompt,negative_prompt,\n",
    "#                        guidance_scale=guidance_scale,strength=strength,lora_scale=lora_scale,num_steps=num_steps,num_imgs=num_imgs,img_shape=img_shape,save_quality=save_quality,\n",
    "#                        face_front=face_front)\n",
    "\n",
    "# # generated_img = perform_diffusion(pipe,path_results,seed,prompt,negative_prompt,\n",
    "# #                        guidance_scale=guidance_scale,strength=strength,lora_scale=lora_scale,num_steps=num_steps,num_imgs=num_imgs,img_shape=img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_multi(generated_img,figsize=(15,10))\n",
    "# display_multi(generated_img[-1],figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.to('cpu');\n",
    "# pipe.to(device);\n",
    "# pipe.enable_sequential_cpu_offload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.get_active_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.set_adapters(['photo_lora', 'toy_face', 'pixel_art'], adapter_weights=[0, 1.0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"toy_face of a hacker with a hoodie\"\n",
    "\n",
    "# clear_memory()\n",
    "# lora_scale = 0.9\n",
    "# image = pipe(\n",
    "#     prompt, num_inference_steps=30, cross_attention_kwargs={\"scale\": lora_scale}, generator=torch.manual_seed(0)\n",
    "# ).images[0]\n",
    "\n",
    "# clear_memory()\n",
    "# image.resize((256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Adapter Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tencent-ailab/IP-Adapter/wiki/IP%E2%80%90Adapter%E2%80%90Face\n",
    "\n",
    "https://github.com/tencent-ailab/IP-Adapter/issues/266\n",
    "\n",
    "https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/195#issuecomment-1905120670\n",
    "\n",
    "***Ip adapter***:\n",
    "    adapter + image \n",
    "\n",
    "***Ip adapter plus***: (CLIP embeddings): \n",
    "    Image encoder is loaded automatically or you can provide it with **CLIPVisionModelWithProjection**\n",
    "\n",
    "***Ip adapter face-id***: (Insightface embeddings)  (uses LORA for consistency)\n",
    "    ```\n",
    "    If you want to use one of the two IP-Adapter FaceID Plus models, you must also load the CLIP image encoder, as this models use both insightface and CLIP image embeddings to achieve better photorealism.\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ip Adapter 1 (multiple images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "face_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png\")\n",
    "# style_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\n",
    "# style_images = [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]\n",
    "\n",
    "face_images_folder = \"/home/waleed/Downloads/face/\"\n",
    "face_images_paths = read_paths(face_images_folder)\n",
    "face_images = [load_img_pil(pth) for pth in face_images_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc_utils import clear_memory\n",
    "import diffusers\n",
    "base_model_path = '../my_testing/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors'\n",
    "# Load SDXL pipeline\n",
    "\n",
    "clear_memory()\n",
    "pipe = diffusers.StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.to(device);\n",
    "# pipe.enable_model_cpu_offload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.load_ip_adapter(\n",
    "  \"h94/IP-Adapter\",\n",
    "  subfolder=\"sdxl_models\",\n",
    "  weight_name=[\"ip-adapter_sdxl.bin\"]\n",
    ")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_ip_adapter_scale([0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image, DDIMScheduler\n",
    "# pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     image_encoder=image_encoder,\n",
    "# )\n",
    "\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.to(device);\n",
    "pipe.enable_model_cpu_offload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0 \n",
    "seed = torch.seed()\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "\n",
    "clear_memory()\n",
    "image = pipe(\n",
    "    prompt=\"woman in white shirt and black pant\",\n",
    "    ip_adapter_image=[face_images],\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=30, num_images_per_prompt=1,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ip adapter 2 (face embeddings insightface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/waleed/Downloads/pic.jpeg'\n",
    "img_path = '/home/waleed/Downloads/face/2025-04-30_16-50-09_9714.png'\n",
    "all_paths = [img_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from insightface.app import FaceAnalysis\n",
    "# import torch\n",
    "\n",
    "# app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "# app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "# image = cv2.imread(img_path)\n",
    "# faces = app.get(image)\n",
    "\n",
    "# faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "\n",
    "\n",
    "import cv2\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "import torch\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_l\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "faces = app.get(image)\n",
    "\n",
    "faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "face_image = face_align.norm_crop(image, landmark=faces[0].kps, image_size=224) # you can also segment the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_multi(face_image,figsize=(5,5),bgr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL, StableDiffusionXLPipeline\n",
    "from PIL import Image\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = '../my_testing/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors'\n",
    "# Load SDXL pipeline\n",
    "\n",
    "clear_memory()\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.enable_model_cpu_offload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ip_adapter\n",
    "import ip_adapter.ip_adapter_faceid_separate\n",
    "# ip_ckpt = \"/home/waleed/my_stuff/pretrained_models/diffusion/ip_adapters/ip-adapter_sdxl.bin\" #\"ip-adapter-faceid-portrait_sdxl.bin\"\n",
    "ip_ckpt = '/home/waleed/.cache/huggingface/hub/models--h94--IP-Adapter-FaceID/snapshots/43907e6f44d079bf1a9102d9a6e56aef7a219bae/ip-adapter-faceid-portrait_sdxl.bin'\n",
    "ip_model = ip_adapter.ip_adapter_faceid_separate.IPAdapterFaceIDXL(pipe, ip_ckpt, device, num_tokens=16, n_cond=5)\n",
    "\n",
    "# ip_ckpt = '/home/waleed/.cache/huggingface/hub/models--h94--IP-Adapter/snapshots/018e402774aeeddd60609b4ecdb7e298259dc729/sdxl_models/ip-adapter_sdxl.bin'\n",
    "# encoder_path = '/home/waleed/.cache/huggingface/hub/models--h94--IP-Adapter/snapshots/018e402774aeeddd60609b4ecdb7e298259dc729/sdxl_models/image_encoder'\n",
    "# ip_model = ip_adapter.ip_adapter.IPAdapterXL(pipe,encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_model.pipe.enable_model_cpu_offload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image\n",
    "prompt = \"portrait of a person in a room, realistic, 4k, high detail\"\n",
    "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "\n",
    "clear_memory()\n",
    "images = ip_model.generate(\n",
    "    # pil_image=image,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    faceid_embeds=faceid_embeds,  \n",
    "    num_samples=1,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    num_inference_steps=30,\n",
    "    # seed=2023\n",
    ")\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ip adapter 3 (multiple adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image, DDIMScheduler\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "base_model_path = all_models[-2]\n",
    "clear_memory()\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    # scheduler=noise_scheduler,\n",
    "    variant=\"fp16\",\n",
    "    image_encoder=image_encoder\n",
    ")\n",
    "clear_memory()\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionAdapterPipeline, MultiAdapter, T2IAdapter\n",
    "# from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlusXL\n",
    "\n",
    "# pipe.load_ip_adapter(\n",
    "#   \"h94/IP-Adapter\",\n",
    "#   subfolder=\"sdxl_models\",\n",
    "#   weight_name=[\"ip-adapter-plus_sdxl_vit-h.safetensors\", \"ip-adapter-plus-face_sdxl_vit-h.safetensors\"]\n",
    "# #   weight_name=[\"ip-adapter-faceid-plusv2_sdxl.bin\", \"ip-adapter-plus-face_sdxl_vit-h.safetensors\"]\n",
    "# )\n",
    "# pipe.set_ip_adapter_scale([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionAdapterPipeline, MultiAdapter, T2IAdapter\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlusXL\n",
    "\n",
    "pipe.load_ip_adapter(\n",
    "  \"h94/IP-Adapter\",\n",
    "  subfolder=\"sdxl_models\",\n",
    "  weight_name=[\"ip-adapter-plus_sdxl_vit-h.safetensors\"]\n",
    ")\n",
    "\n",
    "pipe.set_ip_adapter_scale([0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlusXL\n",
    "\n",
    "encoder_path = '/home/waleed/.cache/huggingface/hub/models--h94--IP-Adapter/snapshots/018e402774aeeddd60609b4ecdb7e298259dc729/models/image_encoder/model.safetensors'\n",
    "ip_ckpt = '/home/waleed/my_stuff/pretrained_models/diffusion/ip_adapters/ip-adapter-faceid-plusv2_sdxl.bin'\n",
    "\n",
    "ip_model = IPAdapterFaceIDPlusXL(pipe, encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.enable_model_cpu_offload();\n",
    "pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/women_input.png\")\n",
    "# # face_image = load_image('/home/waleed/Downloads/face/2025-04-30_16-50-09_9714.png')\n",
    "# style_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\n",
    "# style_images = [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]\n",
    "\n",
    "############################################################\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "face_image = load_image(\"/home/waleed/Downloads/face/face/2025-04-30_16-50-20_8694.png\")\n",
    "# style_folder = \"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/style_ziggy\"\n",
    "# style_images = [load_image(f\"{style_folder}/img{i}.png\") for i in range(10)]\n",
    "\n",
    "style_folder = \"/home/waleed/Downloads/face/face/\"\n",
    "style_images = [load_img_pil(pth) for pth in read_paths(style_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.set_ip_adapter_scale([0.1, 0.9])\n",
    "pipe.set_ip_adapter_scale([0.8, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0#torch.seed()\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "\n",
    "image = pipe(\n",
    "    prompt=\"\",\n",
    "    ip_adapter_image=[style_images, face_image],\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    generator=generator,\n",
    "    height=1024,width=1024\n",
    "    \n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ip adapter 4 (masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import IPAdapterMaskProcessor\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "mask1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask1.png\")\n",
    "mask2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_mask2.png\")\n",
    "\n",
    "output_height = 1024\n",
    "output_width = 1024\n",
    "\n",
    "processor = IPAdapterMaskProcessor()\n",
    "masks = processor.preprocess([mask1, mask2], height=output_height, width=output_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_image1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl1.png\")\n",
    "face_image2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ip_mask_girl2.png\")\n",
    "\n",
    "ip_images = [[face_image1, face_image2]]\n",
    "\n",
    "masks = [masks.reshape(1, masks.shape[0], masks.shape[2], masks.shape[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image, DDIMScheduler\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionXLPipeline,AutoPipelineForText2Image\n",
    "base_model_path = '../my_testing/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors'\n",
    "clear_memory()\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    # scheduler=noise_scheduler,\n",
    "    variant=\"fp16\",\n",
    "    image_encoder=image_encoder\n",
    ")\n",
    "clear_memory()\n",
    "\n",
    "# pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     image_encoder=image_encoder,\n",
    "# )\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=[\"ip-adapter-plus-face_sdxl_vit-h.safetensors\"])\n",
    "pipe.set_ip_adapter_scale([[0.7, 0.7]])  # one scale for each image-mask pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.enable_model_cpu_offload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "num_images = 2\n",
    "\n",
    "image = pipe(\n",
    "    prompt=\"2 girls\",\n",
    "    ip_adapter_image=ip_images,\n",
    "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
    "    num_inference_steps=20,\n",
    "    num_images_per_prompt=num_images,\n",
    "    generator=generator,\n",
    "    cross_attention_kwargs={\"ip_adapter_masks\": masks}\n",
    ").images\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0]\n",
    "# image[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ip adapter 5 (from github issue) \n",
    "https://github.com/tencent-ailab/IP-Adapter/issues/266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL,StableDiffusionXLPipeline\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlusXL,IPAdapterFaceIDPlus\n",
    "\n",
    "def load_model(model_name,isSDXL,image_encoder_path,ip_ckpt,device=None):\n",
    "    noise_scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=1000,\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        clip_sample=False,\n",
    "        set_alpha_to_one=False,\n",
    "        steps_offset=1,\n",
    "    )\n",
    "    vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "    if isSDXL:\n",
    "        vae_model_path = \"stabilityai/sdxl-vae\"\n",
    "    vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "    if isSDXL:\n",
    "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            vae=vae,\n",
    "            scheduler=noise_scheduler,\n",
    "            add_watermarker=False,\n",
    "        ).to(device)\n",
    "    else:\n",
    "        # Load model based on the selected model name\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            scheduler=noise_scheduler,\n",
    "            vae=vae,\n",
    "            feature_extractor=None,\n",
    "            safety_checker=None\n",
    "        ).to(device)\n",
    "\n",
    "    if isSDXL:\n",
    "        # image_encoder_path = \"h94/IP-Adapter/models/image_encoder\"\n",
    "        # ip_ckpt = \"adapters/ip-adapter-faceid-plusv2_sdxl.bin\"\n",
    "        ip_model = IPAdapterFaceIDPlusXL(pipe,image_encoder_path, ip_ckpt, device)\n",
    "    else:\n",
    "        # image_encoder_path = \"h94/IP-Adapter/models/image_encoder\"\n",
    "        # ip_ckpt = \"adapters/ip-adapter-faceid-plusv2_sd15.bin\"\n",
    "        ip_model = IPAdapterFaceIDPlus(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "    return ip_model\n",
    "\n",
    "# Function to process image and generate output\n",
    "def generate_image(input_image, positive_prompt, negative_prompt, width, height, ip_model, num_inference_steps, seed, num_images, batch_size):\n",
    "    saved_images = []\n",
    "    \n",
    "    app = FaceAnalysis(\n",
    "        name=\"buffalo_l\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    "    )\n",
    "    app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "    faces = app.get(input_image)\n",
    "    if not faces:\n",
    "        raise ValueError(\"No faces found in the image.\")\n",
    "\n",
    "    faceid_embeds = torch.from_numpy(faces[0].normed_embedding).unsqueeze(0)\n",
    "    face_image = face_align.norm_crop(input_image, landmark=faces[0].kps, image_size=224)\n",
    "\n",
    "    for image_index in range(num_images):\n",
    "\n",
    "        # Generate the image with the new parameters\n",
    "        generated_images = ip_model.generate(\n",
    "            prompt=positive_prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            faceid_embeds=faceid_embeds,\n",
    "            face_image=face_image,\n",
    "            num_samples=batch_size,\n",
    "            # shortcut=enable_shortcut,\n",
    "            # s_scale=s_scale,\n",
    "            width=width,\n",
    "            height=height,\n",
    "\t\t\t# guidance_scale=cfg,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            # seed=seed,\n",
    "        )\n",
    "\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'stabilityai/stable-diffusion-xl-base-1.0'\n",
    "# image_encoder_path = \"h94/IP-Adapter/models/image_encoder\"\n",
    "# ip_ckpt = \"/home/waleed/Downloads/ip-adapter-faceid-plusv2_sdxl.bin\"\n",
    "# # ip_ckpt = '/home/waleed/.cache/huggingface/hub/models--h94--IP-Adapter/snapshots/018e402774aeeddd60609b4ecdb7e298259dc729/models/image_encoder/model.safetensors'\n",
    "\n",
    "\n",
    "# device = 'cuda'\n",
    "# isSDXL = True\n",
    "# ip_model = load_model(model_name, isSDXL,image_encoder_path,ip_ckpt,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, AutoencoderKL,StableDiffusionXLPipeline\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDPlusXL,IPAdapterFaceIDPlus\n",
    "\n",
    "from ip_adapter.ip_adapter_faceid import IPAdapterFaceIDXL\n",
    "\n",
    "# base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"#\"RunDiffusion/Juggernaut-XL-v8\"#\"SG161222/RealVisXL_V3.0\"\n",
    "base_model_path = all_models[-2]\n",
    "ip_ckpt = \"/home/waleed/Downloads/ip-adapter-faceid-plusv2_sdxl.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=1000,\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        clip_sample=False,\n",
    "        set_alpha_to_one=False,\n",
    "        steps_offset=1,\n",
    "    )\n",
    "\n",
    "vae_model_path = \"stabilityai/sdxl-vae\"\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    add_watermarker=False,\n",
    ")\n",
    "\n",
    "# load ip-adapter\n",
    "# ip_model = IPAdapterFaceIDXL(pipe, ip_ckpt, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder_path = \"h94/IP-Adapter/models/image_encoder\"\n",
    "image_encoder_path = \"/home/waleed/.cache/huggingface/hub/models--h94--IP-Adapter/snapshots/018e402774aeeddd60609b4ecdb7e298259dc729/models/image_encoder/\"\n",
    "ip_ckpt = \"/home/waleed/my_stuff/pretrained_models/diffusion/ip_adapters/ip-adapter-faceid-plusv2_sdxl.bin\"\n",
    "ip_model = IPAdapterFaceIDPlusXL(pipe,image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from insightface.app import FaceAnalysis\n",
    "# from insightface.utils import face_align\n",
    "# input_image = cv2.imread('/home/waleed/Downloads/IMG_20220911_222431.jpg')\n",
    "\n",
    "# generate_image = generate_image(input_image, \"a photo of a person\", \"deformed, unrealistic\", 1024, 1024, ip_model, 30, 0, 1,1)\n",
    "#     # generate_image(input_image, positive_prompt, negative_prompt, width, height, ip_model, num_inference_steps, seed, num_images, batch_size):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import cv2\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "input_image = cv2.imread('/home/waleed/Downloads/IMG_20220911_222431.jpg')\n",
    "\n",
    "seed = torch.seed()\n",
    "# seed = 0\n",
    "\n",
    "num_images = 1\n",
    "batch_size = 1\n",
    "images = generate_image(input_image, \"a photo of a person\", \"deformed, unrealistic\", 1024, 1024, ip_model, 30, seed, num_images,batch_size)\n",
    "    # generate_image(input_image, positive_prompt, negative_prompt, width, height, ip_model, num_inference_steps, seed, num_images, batch_size):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# long prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline,StableDiffusionXLPipeline\n",
    "from utils.misc_utils import *\n",
    "base_model_path = '../my_testing/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors'\n",
    "# base_model_path = '../my_testing/Fooocus/models/checkpoints/realismEngineSDXL_v30VAE.safetensors'\n",
    "# Load SDXL pipeline\n",
    "device = 'cuda'\n",
    "clear_memory()\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "# pipe = DiffusionPipeline.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "#     , torch_dtype       = torch.float16\n",
    "#     , use_safetensors   = True\n",
    "#     , variant           = \"fp16\"\n",
    "# )\n",
    "clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"portrait of a 22-year-old Pakistani woman, fair and glowing complexion, symmetrical face with delicate features, realistic skin texture with faint pores and imperfections, almond-shaped hazel eyes, naturally shaped eyebrows, thin lips, long straight light brown hair with a center part, wearing a light pastel kurta or fashionable modest outfit, calm and confident expression, studio lighting or soft natural light, ultra-detailed, photorealistic, high-resolution, DSLR depth of field, close-up shot, intricate, elegant, highly detailed, very coherent, sharp focus, color paired colors, romantic, pleasing, rich deep vibrant vivid expressive colorful background, great still, dynamic dramatic cinematic atmosphere, ambient aesthetic\"\n",
    "negative_prompt = \"unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label\"\n",
    "\n",
    "# negative_prompt = negative_prompt \n",
    "\n",
    "# prompt = \"(photo-realistic, real photo, masterpiece, best quality, 8k, UHD, RAW photo, levelled out, super resolution, sharp focus, ultra detailed skin, realistic skin, beautiful vivid backgrounds, exquisitely detailed, colorful and intricate details, ray tracing, film lights, spot lights, lustrous skin, physically based rendering, best illustration, best shadow, cinematic lighting, delicate Illustration, official art, aesthetic:1.4), (golden-ratio face, perfect proportioned face, perfectly beautiful), glossy and red lips, (brown eyes), senior high school student, (k-pop idol, miss korea, korean beauty), (short torso, long legs, slim waist, huge hips, huge naturally sagging breasts:1.4), (1girl, solo girl:1.3), (Cinematic view of a wonderful burning castle built in a middle of a dark wood, wide view, fire and ashes, cloudy night:1.3), (Medieval Plate Armour, Full body Armor Suit With Chain Mail :1.3), (Crusader knight, cape, hold a shield and very long sword, fighting, wield a sword:1.4), (carrying a big bow on back:1.2), chastity belt,\"\n",
    "# negative_prompt = \"(worst quality, low quality,low resolution, worst resolution, normal resoultion, collage, bad anatomy of fingers:1.4), korean traditional building, tattoo, watermark, (Dutch angle, Dutch tilt, canted angle, vortex angle, oblique angle:1.3), tiara, (one more girl, nipples:1.4)\"\n",
    "\n",
    "# prompt = \"\"\"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus. \n",
    "# This imaginative creature features the distinctive, bulky body of a hippo, \n",
    "# but with a texture and appearance resembling a golden-brown, crispy waffle. \n",
    "# The creature might have elements like waffle squares across its skin and a syrup-like sheen. \n",
    "# It's set in a surreal environment that playfully combines a natural water habitat of a hippo with elements of a breakfast table setting, \n",
    "# possibly including oversized utensils or plates in the background. \n",
    "# The image should evoke a sense of playful absurdity and culinary fantasy.\n",
    "# \"\"\"\n",
    "\n",
    "# negative_prompt = \"\"\"\\\n",
    "# skin spots,acnes,skin blemishes,age spot,(ugly:1.2),(duplicate:1.2),(morbid:1.21),(mutilated:1.2),\\\n",
    "# (tranny:1.2),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.2),(bad proportions:1.3),\\\n",
    "# extra limbs,(disfigured:1.2),(missing arms:1.2),(extra legs:1.2),(fused fingers:1.5),\\\n",
    "# (too many fingers:1.5),(unclear eyes:1.2),lowers,bad hands,missing fingers,extra digit,\\\n",
    "# bad hands,missing fingers,(extra arms and legs),(worst quality:2),(low quality:2),\\\n",
    "# (normal quality:2),lowres,((monochrome)),((grayscale))\n",
    "# \"\"\"\n",
    "\n",
    "len(prompt),len(negative_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sd_utils import get_prompt_embeddings\n",
    "embed_type = 'sd_emb'\n",
    "(prompt_embeds, prompt_neg_embeds,\n",
    "    pooled_prompt_embeds, negative_pooled_prompt_embeds) = get_prompt_embeddings(pipe,prompt,negative_prompt,embed_type)\n",
    "\n",
    "embed_type = 'lpw'\n",
    "(prompt_embeds_2, prompt_neg_embeds_2,\n",
    "    pooled_prompt_embeds_2, negative_pooled_prompt_embeds_2) = get_prompt_embeddings(pipe,prompt,negative_prompt,embed_type)\n",
    "\n",
    "print(\n",
    "    prompt_embeds.shape,\n",
    "    prompt_neg_embeds.shape,\n",
    "    pooled_prompt_embeds.shape,\n",
    "    negative_pooled_prompt_embeds.shape,\n",
    ")\n",
    "print(\n",
    "    prompt_embeds_2.shape,\n",
    "    prompt_neg_embeds_2.shape,\n",
    "    pooled_prompt_embeds_2.shape,\n",
    "    negative_pooled_prompt_embeds_2.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sd_embed.embedding_funcs\n",
    "# from utils.lpw_stable_diffusion_xl import get_weighted_text_embeddings_sdxl\n",
    "\n",
    "# embeded_prompts = sd_embed.embedding_funcs.get_weighted_text_embeddings_sdxl(pipe,prompt,negative_prompt)\n",
    "# (prompt_embeds, prompt_neg_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds) = embeded_prompts\n",
    "\n",
    "# embeded_prompts = get_weighted_text_embeddings_sdxl(pipe,prompt,negative_prompt)\n",
    "# (prompt_embeds_2, prompt_neg_embeds_2, pooled_prompt_embeds_2, negative_pooled_prompt_embeds_2) = embeded_prompts\n",
    "\n",
    "# print(\n",
    "#     prompt_embeds.shape,\n",
    "#     prompt_neg_embeds.shape,\n",
    "#     pooled_prompt_embeds.shape,\n",
    "#     negative_pooled_prompt_embeds.shape,\n",
    "# )\n",
    "# print(\n",
    "#     prompt_embeds_2.shape,\n",
    "#     prompt_neg_embeds_2.shape,\n",
    "#     pooled_prompt_embeds_2.shape,\n",
    "#     negative_pooled_prompt_embeds_2.shape,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "# num_imgs = 2\n",
    "image = pipe(\n",
    "    prompt_embeds=prompt_embeds,\n",
    "    negative_prompt_embeds=prompt_neg_embeds,\n",
    "    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "    num_inference_steps=30,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(seed)\n",
    ").images\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=30,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(seed)\n",
    ").images\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt_embeds=prompt_embeds_2,\n",
    "    negative_prompt_embeds=prompt_neg_embeds_2,\n",
    "    pooled_prompt_embeds=pooled_prompt_embeds_2,\n",
    "    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds_2,\n",
    "    num_inference_steps=30,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.0,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(seed),num_images_per_prompt=2\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python transformers accelerate insightface\n",
    "import diffusers\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.models import ControlNetModel\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n",
    "\n",
    "# prepare 'antelopev2' under ./models\n",
    "app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "# prepare models under ./checkpoints\n",
    "face_adapter = f'./checkpoints/ip-adapter.bin'\n",
    "controlnet_path = f'./checkpoints/ControlNetModel'\n",
    "\n",
    "# load IdentityNet\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "base_model = 'wangqixun/YamerMIX_v8'  # from https://civitai.com/models/84040?modelVersionId=196039\n",
    "pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n",
    "    base_model,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.cuda()\n",
    "\n",
    "# load adapter\n",
    "pipe.load_ip_adapter_instantid(face_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "face_image = load_image(\"./examples/yann-lecun_resize.jpg\")\n",
    "\n",
    "# prepare face emb\n",
    "face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\n",
    "face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*(x['bbox'][3]-x['bbox'][1]))[-1]  # only use the maximum face\n",
    "face_emb = face_info['embedding']\n",
    "face_kps = draw_kps(face_image, face_info['kps'])\n",
    "\n",
    "# prompt\n",
    "prompt = \"film noir style, ink sketch|vector, male man, highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\"\n",
    "negative_prompt = \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful\"\n",
    "\n",
    "# generate image\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image_embeds=face_emb,\n",
    "    image=face_kps,\n",
    "    controlnet_conditioning_scale=0.8,\n",
    "    ip_adapter_scale=0.8,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install diffusers opencv-python transformers accelerate insightface\n",
    "import diffusers\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import ControlNetModel\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n",
    "\n",
    "# prepare 'antelopev2' under ./models\n",
    "# https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304\n",
    "app = FaceAnalysis(name='antelopev2', root='./', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "# prepare models under ./checkpoints\n",
    "# https://huggingface.co/InstantX/InstantID\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/config.json\", local_dir=\"./checkpoints\")\n",
    "hf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ControlNetModel/diffusion_pytorch_model.safetensors\", local_dir=\"./checkpoints\")\n",
    "hf_hub_download(repo_id=\"InstantX/InstantID\", filename=\"ip-adapter.bin\", local_dir=\"./checkpoints\")\n",
    "\n",
    "face_adapter = './checkpoints/ip-adapter.bin'\n",
    "controlnet_path = './checkpoints/ControlNetModel'\n",
    "\n",
    "# load IdentityNet\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n",
    "\n",
    "base_model = 'wangqixun/YamerMIX_v8'\n",
    "pipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n",
    "    base_model,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# load adapter\n",
    "pipe.load_ip_adapter_instantid(face_adapter)\n",
    "\n",
    "# load an image\n",
    "face_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ai_face2.png\")\n",
    "\n",
    "# prepare face emb\n",
    "face_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\n",
    "face_info = sorted(face_info, key=lambda x:(x['bbox'][2]-x['bbox'][0])*x['bbox'][3]-x['bbox'][1])[-1]  # only use the maximum face\n",
    "face_emb = face_info['embedding']\n",
    "face_kps = draw_kps(face_image, face_info['kps'])\n",
    "\n",
    "# prompt\n",
    "prompt = \"film noir style, ink sketch|vector, male man, highly detailed, sharp focus, ultra sharpness, monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\"\n",
    "negative_prompt = \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful\"\n",
    "\n",
    "# generate image\n",
    "pipe.set_ip_adapter_scale(0.8)\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    image_embeds=face_emb,\n",
    "    image=face_kps,\n",
    "    controlnet_conditioning_scale=0.8,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photomaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "from utils.misc_utils import *\n",
    "from utils.load_and_preprocessing_utils import load_img_pil\n",
    "import PIL\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(\"Device\",device)\n",
    "\n",
    "all_models = [\"stabilityai/stable-diffusion-2-inpainting\",\n",
    "\"diffusionbee/fooocus_inpainting\",\n",
    "\"Vijish/fooocus_inpainting\",\n",
    "\"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "\"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
    "'/home/waleed/my_stuff/codes/my_testing/Fooocus/models/checkpoints/juggernautXL_v8Rundiffusion.safetensors',\n",
    "\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "from photomaker import PhotoMakerStableDiffusionXLPipeline\n",
    "\n",
    "# base_model_path = all_models[-1]\n",
    "# ### Load base model\n",
    "# pipe = PhotoMakerStableDiffusionXLPipeline.from_pretrained(\n",
    "#     base_model_path,  # can change to any base model based on SDXL\n",
    "#     torch_dtype=torch.bfloat16, \n",
    "#     use_safetensors=True, \n",
    "#     variant=\"fp16\",\n",
    "# ).to(device)\n",
    "\n",
    "base_model_path = all_models[-2]\n",
    "### Load base model\n",
    "pipe = PhotoMakerStableDiffusionXLPipeline.from_single_file(\n",
    "    base_model_path,  \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    variant=\"fp16\",\n",
    ")\n",
    "\n",
    "pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# photomaker_path = '/home/waleed/my_stuff/pretrained_models/diffusion/photomaker/photomaker-v1.bin'\n",
    "# photomaker_model = torch.load(photomaker_path, map_location=\"cpu\")\n",
    "\n",
    "# pipe.load_photomaker_adapter(\n",
    "#     photomaker_model,\n",
    "#     subfolder=\"\",\n",
    "#     weight_name='',\n",
    "#     trigger_word=\"img\",\n",
    "#     pm_version='v1'\n",
    "\n",
    "# );\n",
    "\n",
    "photomaker_path = '/home/waleed/my_stuff/pretrained_models/diffusion/photomaker/photomaker-v2.bin'\n",
    "photomaker_model = torch.load(photomaker_path, map_location=\"cpu\")\n",
    "\n",
    "pipe.load_photomaker_adapter(\n",
    "    photomaker_model,\n",
    "    subfolder=\"\",\n",
    "    weight_name='',\n",
    "    trigger_word=\"img\",\n",
    "    pm_version='v2'\n",
    "\n",
    ");\n",
    "\n",
    "pipe.fuse_lora();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# photomaker_path = '/home/waleed/my_stuff/pretrained_models/diffusion/photomaker/photomaker-v1.bin'\n",
    "\n",
    "# ### Load PhotoMaker checkpoint\n",
    "# pipe = PhotoMakerStableDiffusionXLPipeline.from_pretrained(\n",
    "#     base_model_path,  # can change to any base model based on SDXL\n",
    "#     torch_dtype=torch.bfloat16, \n",
    "#     use_safetensors=True, \n",
    "#     variant=\"fp16\"\n",
    "# ).to(device)  \n",
    "\n",
    "# pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "# ### Also can cooperate with other LoRA modules\n",
    "# # pipe.load_lora_weights(os.path.dirname(lora_path), weight_name=lora_model_name, adapter_name=\"xl_more_art-full\")\n",
    "# # pipe.set_adapters([\"photomaker\", \"xl_more_art-full\"], adapter_weights=[1.0, 0.5])\n",
    "\n",
    "# # pipe.fuse_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the input ID images\n",
    "input_folder_name = '/home/waleed/Downloads/face/face/'\n",
    "image_basename_list = os.listdir(input_folder_name)\n",
    "image_path_list = sorted([os.path.join(input_folder_name, basename) for basename in image_basename_list])\n",
    "\n",
    "input_id_images = []\n",
    "for image_path in image_path_list:\n",
    "    input_id_images.append(load_image(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# pip install insightface==0.7.3\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "### \n",
    "# https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/165#issue-2055829543\n",
    "###\n",
    "class FaceAnalysis2(FaceAnalysis):\n",
    "    # NOTE: allows setting det_size for each detection call.\n",
    "    # the model allows it but the wrapping code from insightface\n",
    "    # doesn't show it, and people end up loading duplicate models\n",
    "    # for different sizes where there is absolutely no need to\n",
    "    def get(self, img, max_num=0, det_size=(640, 640)):\n",
    "        if det_size is not None:\n",
    "            self.det_model.input_size = det_size\n",
    "\n",
    "        return super().get(img, max_num)\n",
    "\n",
    "def analyze_faces(face_analysis: FaceAnalysis, img_data: np.ndarray, det_size=(640, 640)):\n",
    "    # NOTE: try detect faces, if no faces detected, lower det_size until it does\n",
    "    detection_sizes = [None] + [(size, size) for size in range(640, 256, -64)] + [(256, 256)]\n",
    "\n",
    "    for size in detection_sizes:\n",
    "        faces = face_analysis.get(img_data, det_size=size)\n",
    "        if len(faces) > 0:\n",
    "            return faces\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "face_detector = FaceAnalysis2(providers=['CPUExecutionProvider', 'CUDAExecutionProvider'], allowed_modules=['detection', 'recognition'])\n",
    "face_detector.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "id_embed_list = []\n",
    "\n",
    "for img in input_id_images:\n",
    "    img = np.array(img)\n",
    "    img = img[:, :, ::-1]\n",
    "    faces = analyze_faces(face_detector, img)\n",
    "    if len(faces) > 0:\n",
    "        id_embed_list.append(torch.from_numpy((faces[0]['embedding'])))\n",
    "\n",
    "\n",
    "id_embeds = torch.stack(id_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([7, 512])\n",
    "# np.shape(id_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the trigger word `img` must follow the class word for personalization\n",
    "# prompt = \"portrait of a woman img shirt and pant, day time, in a busy street, hd, high resolution\"\n",
    "# negative_prompt = \"(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth, grayscale\"\n",
    "\n",
    "prompt = \"portrait of a 22-year-old Pakistani woman img, fair and glowing complexion, symmetrical face with delicate features, realistic skin texture with faint pores and imperfections, almond-shaped hazel eyes, naturally shaped eyebrows, thin lips, long straight light brown hair with a center part, wearing a light pastel kurta or fashionable modest outfit, calm and confident expression, studio lighting or soft natural light, ultra-detailed, photorealistic, high-resolution, DSLR depth of field, close-up shot, intricate, elegant, highly detailed, very coherent, sharp focus, color paired colors, romantic, pleasing, rich deep vibrant vivid expressive colorful background, great still, dynamic dramatic cinematic atmosphere, ambient aesthetic\"\n",
    "negative_prompt = \"cascade image, unrealistic, saturated, high contrast, big nose, painting, drawing, sketch, cartoon, anime, manga, render, CG, 3d, watermark, signature, label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.sd_utils import get_prompt_embeddings\n",
    "# embed_type = 'sd_emb'\n",
    "# (prompt_embeds, prompt_neg_embeds,\n",
    "#     pooled_prompt_embeds, negative_pooled_prompt_embeds) = get_prompt_embeddings(pipe,prompt,negative_prompt,embed_type)\n",
    "\n",
    "\n",
    "## Parameter setting\n",
    "# seed = torch.seed()\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "num_steps = 50\n",
    "style_strength_ratio = 20\n",
    "start_merge_step = int(float(style_strength_ratio) / 100 * num_steps)\n",
    "if start_merge_step > 30:\n",
    "    start_merge_step = 30\n",
    "\n",
    "images = pipe(\n",
    "    # prompt_embeds=prompt_embeds_2,\n",
    "    # negative_prompt_embeds=prompt_neg_embeds_2,\n",
    "    # pooled_prompt_embeds=pooled_prompt_embeds_2,\n",
    "    # negative_pooled_prompt_embeds=negative_pooled_prompt_embeds_2,\n",
    "    # guidance_scale=7.5,\n",
    "    input_id_images=input_id_images,\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_images_per_prompt=2,\n",
    "    num_inference_steps=num_steps,\n",
    "    start_merge_step=start_merge_step,\n",
    "    generator=generator,\n",
    "    id_embeds=id_embeds\n",
    "    \n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
